% qregexp.tex

\input epsf

\font\sf=cmss10
\font\eightrm=cmr8

\def\rx#1{{$^{\scriptscriptstyle\lceil}$\tt#1$\scriptscriptstyle\rfloor$}}

\def\<#1>{\leavevmode\hbox{$\langle$#1\/$\rangle$}}
\def\tok#1{\leavevmode\hbox{\eightrm\uppercase{#1}}}
\def\\{\char`\\}
\def\^{\char`\^}
\def\lex#1{\leavevmode\hbox{\tt#1}}
\let\is=\longrightarrow
\let\alt=\mid

\def\f#1{\strut\enskip#1\hfil\enskip}
\def\g#1{\strut\enskip\hfil#1\hfil\enskip}
\def\x{\phantom{+}}
\def\haha{$\hat a$}

\centerline{\bf The Implementation of Regular Expression in~Qt~3.0}
\smallskip

\beginsection 1. Introduction

The purpose of this text is to describe the algorithms found in the implementation of regular expressions in~Qt~3.0.  We will focus on theory and examples, and mention hastily where to look for the relevant code.

Regular expressions are provided by the class {\sf QRegExp} in files {\it qregexp.cpp} and {\it qregexp.h}.  (We shall use a sans-serif font, {\sf like this}, for symbols in source files.)  A {\sf QRegExp} object is represented by a pair of pointers:  {\sf eng}, a pointer to a (shared) {\sf QRegExpEngine} object, and {\sf priv}, a pointer to the rest of the private data.  What interests us here is primarly {\sf QRegExpEngine}, which encapsulates the regular expression algorithms.

In the text, regular expressions are enclosed in Japanese-style quotes, like this: \rx{abc}.  In the tradition of compilers that compile themselves, regular expressions are used to define formally lexical constructs of regular expressions.

Before going any further, the reader is urged to read Chapter~3 of the ``Dragon Book''---that is, Aho, Ravi, and Ullman's famous {\sl Compilers: Principles, Techniques, and Tools}.

\beginsection 2. Lexical Analysis

{\it Lexical analysis\/} (or {\it tokenizing\/}) of regular expressions consists in recognizing the following tokens:  $$\vbox{\halign{\f{\sf#} \vrule & \f{#} & \f{#} \cr
 \it Identifier       & \it Notation     & \it Value \cr
\noalign{\hrule}
  Tok\_Eos            & \#               & $0$ \cr
  Tok\_Dollar         & \lex\$           & $1$ \cr
  Tok\_LeftParen      & \lex(            & $2$ \cr
  Tok\_MagicLeftParen & \lex{(?:}        & $3$ \cr
  Tok\_PosLookahead   & \lex{(?=}        & $4$ \cr
  Tok\_NegLookahead   & \lex{(?!}        & $5$ \cr
  Tok\_RightParen     & \lex)            & $6$ \cr
  Tok\_CharClass      & \tok{char-class} & $7$ \cr
  Tok\_Caret          & \lex\^           & $8$ \cr
  Tok\_Quantifier     & \tok{quantifier} & $9$ \cr
  Tok\_Bar            & \lex|            & $10$ \cr
  Tok\_Word           & \lex{\\b}        & $11$ \cr
  Tok\_NonWord        & \lex{\\B}        & $12$ \cr
  Tok\_Char           & \tok{char}       & 0x10000--0x1FFFF \cr
  Tok\_BackRef        & \tok{back-ref}   & 0x20001--0x2000E \cr
}}$$  Most tokens stand only for one lexeme, in which case the notation for the token is the lexeme itself.  It is so for \lex\$, \lex(, \lex{(?:}, \lex{(?=}, \lex{(?!}, \lex), \lex\^, \lex|, \lex{\\b}, and \lex{\\B}.  Lexemes for \tok{char-class} either match (grossly) \rx{\\[\\\^?\\]?([\^\\\\]|\tok{escape})*\\]} or are one of \lex{\\d} (digit), \lex{\\D} (non-digit), \lex{\\s} (space), \lex{\\S} (non-space), \lex{\\w} (alphanumeric), \lex{\\W} (non-alphanumeric), and \lex. (anything).  Also, between {\tt[} and {\tt]}, character ranges can be specified like this: \tok{char}~{\tt-}~\tok{char}.  Lexemes for \tok{quantifier} are \lex{*} (0~or more occurrences), \lex{?} (0~or~1~occurrences), \lex{+} (1~or~more occurrences), and all lexemes matching \rx{\\\string{[0-9]*(,[0-9]*)?\\\string}}.  The later specifies lexemes such as \lex{\string{\string}} (0~occurrences), \lex{\string{$x$\string}} ($x$~occurrences), \lex{\string{,\string}} (0~or~more occurrences), \lex{\string{$x$,\string}} ($x$ or more occurrences), \lex{\string{,$y$\string}} (0~to~$y$ occurrences), and \lex{\string{$x$,$y$\string}} ($\min\{x,y\}$~to~$\max\{x,y\}$~occurrences).  Lexemes for \tok{char} are either normal characters (``non-metacharacters'') or escapes (such as \lex{\\n} and \lex{\\\$}).  If the Unicode character to match has value $x$, the corresponding token has value ${\rm 0x10000} + x$.  Finally, lexemes for \tok{back-ref} match \rx{\\[1-9][0-9]*}.

Here's how to use the lexical analyzer:  (1)~Call {\sf startTokenizer} to set up the analyzer with a regular expression; (2)~call {\sf getToken} to obtain the next token; and (3)~call {\sf skipChars} to skip over a certain number of characters in the regular expression.  Four variables complete this interface:  (1)~{\sf yyCharClass} specifies what characters are in a \tok{char-class} (it is actually a pointer, because of an exotic constraint); (2)~{\sf yyMinRep} and (3)~{\sf yyMaxRep} specify the repetition interval for \tok{quantifiers} ({\sf yyMaxRep} is at most {\sf InftyRep}~(1000), which is considered as infinity); (4)~{\sf yyError} is set to {\sf true} whenever a lexical error is encountered.

As an example, lexical analysis of the regular expression \rx{if|([A-Z\_a-z][0-9A-Z\_a-z]\string{0,30\string})} gives the following sequence of tokens:  $$\tok{char}~({\tt i}), \quad \tok{char}~({\tt f}), \quad \lex|, \quad \lex(, \quad \tok{char-class}, \quad \tok{char-class}, \quad \tok{quantifier}, \quad \lex{)}, \quad \#, \quad \#, \quad \ldots$$  Notice that {\sf getToken} constantly returns \# once the end of the string has been reached.

\beginsection 3. Syntactic Analysis

Here is the Backus-Naur Form grammar for regular expressions in~Qt~3.0:  $$\eqalign{
      \<regexp> & \is \<expression>\#\cr
  \<expression> & \is \<term>\lex|\<expression> \alt \<term>\cr
        \<term> & \is \<factor>\<term> \alt \epsilon\cr
      \<factor> & \is \<atom>\>\tok{quantifier} \alt \<atom>\cr
        \<atom> & \is \lex(\>\<expression>\>\lex) \alt \lex{(?:}\>\<expression>\>\lex) \alt \tok{char-class} \alt \tok{char}\cr
                & \mathrel{\phantom{\is}} {\quad} \alt \lex\$ \alt \lex\^ \alt \lex{(?=}\>\<expression>\>\lex) \alt \lex{(?!}\>\<expression>\>\lex) \alt \lex{\\b} \alt \lex{\\B}\cr
}$$  {\it Syntactic analysis\/} (or {\it parsing\/}) is performed using a predictive parser (Section~4.4 in the Dragon Book), with one lookahead token ({\sf yyTok}).  There are two complications that we will see in due time, introduced by general quantifiers (Section~9) and lookaheads (Section~10).  The former ``hiccups'' on the input string; the latter skips parts of it.

\beginsection 4. Construction of an NFA

The Dragon Book presents two algorithms for the construction of an automaton from a pure regular expression.  Section~3.7 of the Dragon Book presents Thompson's construction, whose output is a nondeterministic finite automaton (NFA) with $\epsilon$-transitions.  Section~3.9 presents an algorithm for converting a pure regular expression into a deterministic finite automaton (DFA) using a function called {\it followpos}.

The implementation of {\sf QRegExp} relies on an variant of the procedure of Section~3.9, and the result is an NFA essentially without $\epsilon$-transitions.  For every node $n$ in the syntax tree for the regular expression to parse, we compute {\it minl} (``minimum length''), {\it firstpos}, {\it lastpos}, and {\it followpos\/} according to the following definitions ({\it lastpos\/} is symmetrical with {\it firstpos\/}):  $$\vbox{\halign{\g{#}\vrule && \g{$#$} \cr
  $n$                   & {\it minl}(n)     & {\it firstpos}(n)                 & {\it followpos} \cr
\noalign{\hrule}
  $\epsilon$            & 0                 & \emptyset \cr
  \tok{char}            & 1                 & \{s\} \cr
  \tok{char-class}      & 1                 & \{s\} \cr
  cat: $n_1 \cdot n_2$  & {\it minl}(n_1) +
                          {\it minl}(n_2)   & {\it firstpos}(n_1)
                                              \ [{} \cup {\it firstpos}(n_2)\,] & {\it lastpos}(n_1) \rightarrow
                                                                                  {\it firstpos}(n_2) \cr
  or: $n_1 \mid n_2$    & \min\smash{\bigl\{}{\it minl}(n_1),
                          \ {\it minl}(n_2)\smash{\bigr\}}
                                            & {\it firstpos}(n_1) \cup
                                              {\it firstpos}(n_2) \cr
  plus: $n_1+$          & {\it minl}(n_1)   & {\it firstpos}(n_1)               & {\it lastpos}(n_1) \rightarrow
                                                                                  {\it firstpos}(n_1) \cr
  opt: $n_1?$           & 0                 & {\it firstpos}(n_1) \cr
}}$$  Let `${\it followpos}(\{n_1, \ldots, n_k\})$' stand for `${\it followpos}(n_1) \cup \cdots \cup {\it followpos}(n_k)$'.  Then, we use `$X \rightarrow Y$' meaning `$Y \subseteq {\it followpos}(X)$'.  The unusual bracket construct for ${\it firstpos}(n_1 \cdot n_2)$ means `${\it firstpos}(n_1) \cup {\it firstpos}(n_2)$ if ${\it minl}(n_1) = 0$, otherwise ${\it firstpos}(n_1)$'.

The automata we are constructing have a nice property:  For any state, all in-transitions are labeled with the same input symbol.  We can thus associate the input symbol to the state and have no label on transitions.  The function {\it followpos\/} may then serve as the transition function for an NFA essentially without $\epsilon$-transitions that recognizes the language of the regular expression.

Here is the transition table of the NFA for \rx{(?:ab|a)*}, obtained by calling {\sf dump} on the engine:  $$\vtop{\hsize=0.4\hsize\obeylines
  $0$ (initial)
  \quad match character class $0$
  \quad positive character class
  \quad $\rightarrow$ $1$
  \quad $\rightarrow$ $2$
  \quad $\rightarrow$ $4$
\smallskip
  $1$ (final)
  \quad match character class $1$
  \quad positive character class
\smallskip
  $2$
  \quad match {\tt a}
  \quad $\rightarrow$ $3$
}\qquad\vtop{\hsize=0.4\hsize\obeylines
  $3$
  \quad match {\tt b}
  \quad $\rightarrow$ $1$
  \quad $\rightarrow$ $2$
  \quad $\rightarrow$ $4$
\smallskip
  $4$
  \quad match {\tt a}
  \quad $\rightarrow$ $1$
  \quad $\rightarrow$ $2$
  \quad $\rightarrow$ $4$
}$$  (Some details were removed from the actual output.)  This corresponds to $$\epsfbox{qregexp.1}$$  Section~3.9 of the Dragon Book goes on with a set construction that leads to a DFA, but we stop here.

In practice, {\it followpos\/} can be computed without an explicit syntax tree.  The class {\sf Box} encapsulates the synthetized attributes of a node, namely {\it minl}, {\it firstpos}, and {\it followpos}.  It possesses one constructor for each kind of leaf, namely $\epsilon$, \tok{char}, and \tok{char-class}.  For \tok{char} and \tok{char-class} leaves, a new state~$s$ is created in the automaton.

A {\sf Box} is, really, an abstraction for a regular expression fragment.  The name {\sf Box} suggests the idea of a ``black box.''  Indeed, a {\sf Box} contains only ``surface'' information about the regular expression it represents:  the minimum length, the left states, and the right states.  Regular expressions such as \rx{abc} and \rx{x+y+z+} are represented by the same {\sf Box} (assuming the state numbers correspond).

Boxes support the four combining operations {\sf cat}, {\sf or}, {\sf plus}, and {\sf opt}, and take care of adding transitions to the automaton for {\sf cat} and {\sf plus}.  We can construct an automaton for \rx{(?:ab|a)*} in this way:  $$\hbox{\sf Box b('a'); \quad b.cat(Box('b')); \quad b.or(Box('a')); \quad b.plus(); \quad b.opt();}$$  Or almost so\dots\spacefactor=3000{} It is useful to define a unique initial state~$0$, and a unique final state~$1$ reached using $\epsilon$-transitions.  This is what the procedure {\sf parse} does.  Also, the actual interface of class {\sf Box} is more cumbersome than hinted here.

\beginsection 5. Simulation of an NFA

Section~3.7 of the Dragon Book describes, among other, an algorithm for simulating an NFA with $\epsilon$-transitions using two stacks and a bit vector.  For an NFA without $\epsilon$-transitions, the theoretical algorithm reduces to $$\vbox{
  \+ $S := {\it followpos}(s_0);$ \cr
  \+ ${\it ch} := {\it next\_char}();$ \cr
  \+\bf while\ & ${\it ch} \ne {\bf eof}$ \bf do begin \cr
  \+           & $S := {\it followpos}(S) \cap {\it matchpos}({\it ch});$ \cr
  \+           & ${\it ch} := {\it next\_char}();$ \cr
  \+\bf end \cr
  \+ {\bf return [$f \in S$]}; \cr
}$$  The {\it matchpos\/} function gives the set of states whose label matches $\it ch$.  For instance, in the NFA for \rx{(?:ab|a)*}, we have ${\it matchpos}({\tt a}) = \{1, 2, 4\}$ and ${\it matchpos}({\tt b}) = \{1, 3\}$.  (The final state,~$1$, matches anything, including the NIL character.  The net result is a $\epsilon$-transition.)

In the implementation, we need two stacks capable of storing $N$~integers and one vector of $N$~integers.  One stack represents the set of current states; the other represents the set of next states.  The integer vector also represents the set of next states.

The ``current stack'' is represented by the variables {\sf mmCurStack} and {\sf ncur}, so that {\sf mmCurStack[0]}, \dots, {\sf mmCurStack[ncur${} - {}$1]} are the {\sf ncur} distinct current states.  The ``next stack'' is implemented in a similar fashion with variables {\sf mmNextStack} and {\sf nnext}.  The ``bit vector'' is represented by an array {\sf mmInNextStack} of $N$ integers such that $$\hbox{\sf mmInNextStack[$i$]} = \cases{
  j,  & \hbox{if there exists $j$ such that $\hbox{\sf mmNextStack[$j$]} = i$;}\cr
  -1, & \hbox{otherwise.}\cr
}$$  If the current states are $\{1, 2, 4\}$ and the next states on reading {\tt a} are $\{1, 3\}$, one possible configuration is $$\vbox{\halign{ \g{#}\vrule && \g{#} \cr
  \sf ch  & \sf mmCurStack    & \sf mmNextStack   & \sf mmInNextState \cr
\noalign{\hrule}
  {\tt b} & $[1, 2, 4]$       & $[1, 3]$          & $[-1, \x0, -1, \x1, -1]$ \cr
}}$$  Let's simulate the execution of the automaton for \rx{(?:ab|a)*} on input {\tt aabaxyz}:  $$\vbox{\halign{ \g{#}\vrule && \g{#} \cr
  \sf ch  & \sf mmCurStack    & \sf mmNextStack   & \sf mmInNextState \cr
\noalign{\hrule}
  {\tt a} & $[0]$             & $[1, 2, 4]$       & $[ -1, \x0, \x1,  -1, \x2]$ \cr
  {\tt a} & $[1, 2, 4]$       & $[1, 2, 4]$       & $[ -1, \x0, \x1,  -1, \x2]$ \cr
  {\tt b} & $[1, 2, 4]$       & $[1, 3]$          & $[ -1, \x0,  -1, \x1,  -1]$ \cr
  {\tt a} & $[1, 3]$          & $[1, 2, 4]$       & $[ -1, \x0, \x1,  -1, \x2]$ \cr
  {\tt x} & $[1, 2, 4]$       & $[1]$             & $[ -1, \x0,  -1,  -1,  -1]$ \cr
  {\tt y} & $[1]$             & $[\,]$            & $[ -1,  -1,  -1,  -1,  -1]$ \cr
}}$$  At every step, the string read so far (without the current {\sf ch}) is matched by \rx{(?:ab|a)*} if and only if $\hbox{\sf mmInNextState}[1] \ge 0$.  Thus, $\epsilon$, {\tt a}, {\tt aa}, {\tt aab}, and {\tt aaba} belong to the language of \rx{(?:ab|a)*}.  If minimal matching is enabled, the procedure stops as soon as it finds out that $\epsilon$ matches; otherwise, the procedure continues until $\hbox{\sf mmCurStack}$ is $[\,]$ or $[1]$, and then reports ${\tt aaba}$ as the longest match.

\beginsection 6. Bad-character Heuristic

The previous section showed how \rx{(?:ab|a)*} matches {\tt aabaxyz}.  Generally, we want to match {\it in\/} a string---for example, match \rx{xyz} in {\tt abcdef}$\ldots${\tt wxyz}.  The usual procedure is:  Simulate the NFA for \rx{xyz} on {\tt abcdef}$\ldots${\tt wxyz}, then ``slide'' and try again on {\tt bcdef}$\ldots${\tt wxyz}, then try again on {\tt cdef}$\ldots${\tt wxyz}, \dots, then on {\tt wxyz}, then on {\tt xyz} to finally get a match.

To avoid most of these NFA simulations, we use a heuristic inspired from the ``bad-character'' heuristic of the Boyer--Moore algorithm (see Section~34.5 of {\sl Introduction to Algorithms\/} by Cormen, Leiserson, and Rivest).  By looking at \rx{xyz}, we can derive the following information about any string $w$ that matches:  (1)~{\tt x} does not occur before position~$0$ in $w$; (2)~{\tt y} does not occur before position~$1$ in $w$; (3)~{\tt z} does not occur before position~$2$ in $w$; (4)~any other character does not occur at all in $w$; and (5)~$w$ is at least $3$-character long.  This information can be represented by a table {\sf occ1} (``first occurrence'') such that $$\hbox{\sf occ1[{\it ch\/}]} = \cases{
  j,      & \hbox{if {\it ch} first can occur at position $j$ in $w$;} \cr
  \infty, & \hbox{if {\it ch} cannot occur at all in $w$;}\cr
}$$ and by an integer {\sf mlen} giving the minimum length of $w$.  For \rx{xyz}, we have $$\hbox{\sf occ1}[{\tt x}] = 0 \qquad \hbox{\sf occ1}[{\tt y}] = 1 \qquad \hbox{\sf occ1}[{\tt z}] = 2 \qquad \hbox{\sf occ1}[\hbox{\rm anything but {\tt x}, {\tt y}, {\tt z}}] = \infty,$$ and $\hbox{\sf mlen} = 3$.

Let's now suppose we want to match \rx{xyz} in {\tt fxzyyxyzz}.  We will use a table {\sf slideTab} of $3$~integers such that $\hbox{\sf slideTab[$i$]} = j$ if, in $i$ turns from now, we may skip $j$ positions.  (We will soon see what that really means.)  Before doing anything else, we initialize {\sf slideTab} with zeros and we look at the first~$3$~characters.  Since $\hbox{\sf occ1[{\tt f}]} = \infty$, we may, this turn, skip one position---we set $\hbox{\sf slideTab}[0] = 1$ accordingly, but stay in place for the moment.  The {\tt x} causes no problem, because a {\tt x} can appear anywhere.  The {\tt z} appears in the first position possible for this turn (a {\tt z} cannot occur before position~$2$), so next turn we may skip $2$~positions to skip over the {\tt z}---we set $\hbox{\sf slideTab}[1] = 2$.  The net result is $\hbox{\sf slideTab} = [1, 2, 0]$.

Let's start!  Since $\hbox{\sf slideTab}[0] = 1$, we skip one position now from {\tt fxzyyxyzz} to {\tt xzyyxyzz}.  One turn has passed; all the {\sf slideTab} entries are shifted to the left and $\hbox{\sf slideTab}[2]$ is set to zero.  We now have $\hbox{\sf slideTab} = [2, 0, 0]$.  We also consider a new character, {\tt y}, in position~$2$:  We set $\hbox{\sf slideTab}[2] = 1$, to get $\hbox{\sf slideTab} = [2, 0, 1]$.  Again, we don't need to simulate the NFA, because we may skip $2$~positions (to get rid of the~{\tt z}).  These two positions are skipped one by one, by transforming $\hbox{\sf slideTab}$ from $[2, 0, 1]$ to $[1, 1, 1]$---informally, both mean the same thing.  Since $\hbox{\sf slideTab}[0] = 1$, we skip one position now from {\tt xzyyxyzz} to {\tt zyyxyzz}, and by shifting {\sf slideTab} w obtain $[1, 1, 0]$.  The second {\tt y} makes us set $\hbox{\sf slideTab}[2]$ to~$1$; we obtain $[1, 1, 1]$.

We'll go faster now.  From {\tt zyyxyzz} $[1, 1, 1]$, we skip to {\tt yyxyzz} $[1, 1, 0]$, then to {\tt yxyzz} $[1, 0, 1]$, then to {\tt xyzz} $[0, 2, 0]$.  Since $\hbox{\sf slideTab}[0] = 0$ (``this turn, don't slide''), we must now simulate the automaton.  The automaton finds a match with {\tt xyz}.

For the example, we have supposed that {\sf slideTab} is indexed from~$0$~to $\hbox{\sf mlen} - 1$.  The actual implementation in procedure {\sf match} uses a circular buffer for efficiency.

We have assumed all along that an~{\sf occ1} table was available, but it's now the time to see how to compute one from any regular expression.  This is accomplished at the same time as the computations for {\it firstpos} and friends, according to the following table:  $$\vbox{\halign{\g{#}\vrule && \g{$#$} \cr
  $n$                   & {\it firstocc}(n)[i] \cr
\noalign{\hrule}
  $\epsilon$            & \infty \cr
  \tok{char}            & \hbox{$0$ for a matching $i$; $\infty$ otherwise} \cr
  \tok{char-class}      & \hbox{$0$ for a matching $i$; $\infty$ otherwise} \cr
  cat: $n_1 \cdot n_2$  & \min\smash{\bigl\{}{\it firstocc}(n_1)[i],\ {\it minl}(n_1) + {\it firstocc}(n_2)[i]\smash{\bigr\}} \cr
  or: $n_1 \mid n_2$    & \min\smash{\bigl\{}{\it firstocc}(n_1)[i],\ {\it firstocc}(n_2)[i]\smash{\bigr\}} \cr
  plus: $n_1+$          & {\it firstocc}(n_1)[i] \cr
  opt: $n_1?$           & {\it firstocc}(n_1)[i] \cr
}}$$  Class {\sf Box} does that for free.  Also, for economy, the $65536$~possible characters are grouped in equivalence classes modulo {\sf NumBadChars} ($128$).

\beginsection 7. Good-string Heuristic

If any match has to contain a literal substring at a more-or-less known position, we can do significantly faster than with the bad-character heuristic using what we'll call the ``good-string heuristic'' of Perl.

By looking at \rx{(a|bc)(def)+}, we can deduce that {\tt def} has to appear at position $1$~or~$2$ of any match (e.g.,~{\tt adef} or {\tt bcdef}).  Given that information, we can look for {\tt def} in the target string.

The information is gathered, as usual, in the {\sf Box} class.

\beginsection 8. Text Capture

To enable text capture, one just needs to call {\sf startAtom} before parsing an atom ({\sf startAtom(true)} if the atom is to be captured, otherwise {\sf setAtom(false)}) and {\sf finishAtom} after.  These procedures also assign a sequence number to each atom.  For example, the regular expression \rx{a((bc)d)} contains seven atoms:  $$\hbox{\rx{$\underbrace{\underbrace{\tt a}_1\underbrace{{\tt (}\underbrace{{\tt (}\underbrace{\tt b}_4\underbrace{\tt c}_5{\tt )}}_3\underbrace{\tt d}_6{\tt )}}_2}_0$}}$$  By calling {\sf dump} on the NFA, we obtain that information in the following form: $$\vtop{\halign{\g{$#$} \vrule & \g{$#$}\cr
  {\it State\/} & {\it Atom\/} \cr
\noalign{\hrule}
  0             & 0 \cr
  1             & 0 \cr
  2             & 1 \cr
  3             & 4 \cr
  4             & 5 \cr
  5             & 6 \cr
}}\qquad\vtop{\halign{\g{$#$} \vrule && \g{$#$} \cr
  {\it Atom\/} & {\it Parent\/} & {\it Capture\/} \cr
\noalign{\hrule}
  0            &  -1            & -1 \cr
  1            &  0             & -1 \cr
  2            &  0             &  0 \cr
  3            &  2             &  1 \cr
  4            &  3             & -1 \cr
  5            &  3             & -1 \cr
  6            &  2             & -1 \cr
}}$$  The column labeled ``Capture'' contains the number of the capture zone:  {\tt ((bc)d)} has number~$0$ and {\tt (bc)} has number~$1$.

While matching, a ``longest-leftmost'' rule applies:  The first capture zone should be ``longest leftmost,'' and the second one within the limits imposed by the first match.  Thus both \rx{a*(a*)} and \rx{(a*)a*} capture {\tt aaa} on input {\tt aaa}.  This might be unfortunate, but there's no easy way to change that.

The rest of the algorithm is explained by comments in the source code (see the function {\sf testMatch}).

\beginsection 9. General Quantifiers

A quantifier is characterized by a pair of numbers~$(x, y)$, where $x$~is a nonnegative integer and $y$~is a nonnegative integer or infinity, with the constraint $x \le y$.  These numbers denote the minimum and maximum number of occurrences of a construct.  The special cases $(0, 1)$~and~$(1, \infty)$ are called ``opt''~and~``plus,'' respectively; these are the only one we have seen so far.

There are essentially two ways of implementing general quantifiers:  Teach the automata to count, or replace \rx{$a$\string{$x$,$y$\string}} with something like $$\hbox{\rx{$\underbrace{\hbox{$aa$}\ldots\hbox{$a$\vphantom)}}_{x\rm\ times}\,\,\underbrace{\!\hbox{($a$($a$(}\ldots\hbox{$a$?}\ldots\hbox{)?)?)?}\!}_{y\rm\ times}\,$}.}$$  The second approach needs larger transition tables, but is simpler to implement and makes matching slightly faster, and thus was chosen.

Because of text capture, things cannot be as simple as suggested above.  Given an atom~$a$, \rx{$a$\string{$x$,$y$\string}} with $x \ne 0$ and $y \ne \infty$ is equivalent to $$\hbox{\rx{$\underbrace{\hbox{\haha\haha}\ldots\hbox{\haha\vphantom)}}_{\alpha\rm\ times}\,\,\underbrace{\!\hbox{(\haha(?:\haha(?:}\ldots\hbox{\haha?}\ldots\hbox{)?)?)?}\!}_{\beta\rm\ times}\,\,a$}} \quad \hbox{with} \quad \alpha = x - 1,\ \beta = y - x,$$ where \haha~is the same as $a$ except that left parentheses are replaced with ``magic left parentheses'' to avoid capture.  If $x = 0$, $\alpha$ is taken as~$0$ and the whole expression is enclosed within ({\tt (?:} and {\tt )?}).  If $y = \infty$, $\beta$ is taken as~$0$ and the $a$ at the end is followed by {\tt +}.  Notice that text capture behaves correctly.

A natural way to replace \rx{(a(b|c)+)\string{2,4\string}} with $$\hbox{\rx{(?:a(?:b|c)+)$\,$(?:(?:a(?:b|c)+)(?:a(?:b|c)+)?)?$\,$(a(b|c)+)}}$$ is to have a preprocessor.  Indeed, this is the way wildcard patterns are handled in {\sf QRegExp}.  But this requires an extra pass on the data.  The solution is the ``hiccup'' method:  Before parsing an atom $a$, the parser state is saved in local variables.  If the atom is followed by a non-trivial quantifier, the state is restored (using {\sf YYREDO}) to what it was before and the atom is parsed again, thus creating new states.  This is what we call a ``hiccup.''  Precisely $\alpha + \beta$ hiccups are necessary.  For the time of a hiccup, the variable {\sf yyCapture} is set to {\sf false}, so that all parentheses in $a$ become magical.  The result is an automaton identical to that we would have had with a preprocessor, without having to dirty our hands with search-and-replace.

\beginsection 10. Anchors

An anchor is a zero-length assertion about the input string.  The anchors are {\tt\^}~(beginning of input), {\tt\$}~(end of input), {\tt\\b}~(word boundary), {\tt\\B}~(word non-boundary), {\tt(?=}$\ldots${\tt)}~(positive lookahead), and {\tt(?!}$\ldots${\tt)}~(negative lookahead).

In general, anchors are attached to the transitions of an automaton.  Here is, for illustration, the automaton for \rx{(?:a*\$b+)+}:  $$\epsfbox{qregexp.2}$$  (Incidentally, it will never match.)  We saw earlier that the input symbols can be stored with the states instead of with the transitions.  This is generally impossible for anchors.

The {\sf Box} class handles anchors according to three functions.  The first one is {\it leftanchors\/}:  $$\vbox{\halign{\g{#}\vrule && \g{$#$} \cr
  $n$                   & {\it leftanchors}(n)[m] \cr
\noalign{\hrule}
  $\epsilon$            & T \cr
  \tok{char}            & T \cr
  \tok{char-class}      & T \cr
  cat: $n_1 \cdot n_2$  & {\it leftanchors}(n_1)[m] \ [{} \land {\it skipanchors}(n_1) \land {\it leftanchors}(n_2)[m]\,] \cr
  or: $n_1 \mid n_2$    & {\it leftanchors}(n_1)[m] \lor {\it leftanchors}(n_2)[m] \cr
  plus: $n_1+$          & {\it leftanchors}(n_1)[m] \cr
  opt: $n_1?$           & {\it leftanchors}(n_1)[m] \cr
}}$$  The function {\it rightanchors\/} is symmetrical with {\it leftanchors} and is omitted here.  Finally, here is {\it skipanchors\/}:  $$\vbox{\halign{\g{#}\vrule && \g{$#$} \cr
  $n$                   & {\it skipanchors}(n) \cr
\noalign{\hrule}
  $\epsilon$            & T \cr
  \tok{char}            & T \cr
  \tok{char-class}      & T \cr
  cat: $n_1 \cdot n_2$  & [\,{\it skipanchors}(n_1)[m] \land {\it skipanchors}(n_2)[m]\,] \cr
  or: $n_1 \mid n_2$    & {\it skipanchors}(n_1)[m] \lor {\it skipanchors}(n_2)[m] \cr
  plus: $n_1+$          & {\it skipanchors}(n_1)[m] \cr
  opt: $n_1?$           & T \cr
}}$$  For cat-nodes, transitions from ${\it lastpos}(n_1)$ to ${\it firstpos}(n_2)$ bear ${\it rightanchors}(n_1) \land {\it leftanchors}(n_2)$.  For plus-nodes, transitions from ${\it lastpos}(n_1)$ to ${\it firstpos}(n_1)$ bear ${\it rightanchors}(n_1) \land {\it leftanchors}(n_1)$.

The operations $\land$ and $\lor$ are implemented by procedures {\sf anchorConcatenation} and {\sf anchorAlternation}, respectively.

The regular expression that constitutes the lookahead are parsed using a brand new {\sf QRegExpEngine} object.  For example, when parsing \rx{a(?=b)c}, the first tokens are \tok{char} ({\tt a}) and \lex{(?=}; then {\tt b} is parsed with NFA and is skipped over using {\sf skipChars}; then \lex{)} and \lex{c} are read.

\beginsection 11. Start-of-input Optimization

If all the out-transitions of the initial state bear a {\tt\^}-anchor, we know that the automaton won't match anything that doesn't start at index~$0$, so there is no need to slide the input string if there is no match at the first position.

\beginsection 12. Back-references

With back-references, we leave the realm of regular expressions.  Indeed, even context-free grammars cannot handle back-references.  Still, back-references are easy to support, once we've got this far.

The basic idea is to unify back-references with the rest of the regular expression engine.  Let's consider the regular expression \rx{(a*)b\\1}.  We construct the following automaton:  $$\epsfbox{qregexp.3}$$  State~$2$ represents the {\tt a} of \rx{(a*)b\\1}, state~$3$ the {\tt b}, and state~$4$ the {\tt \\1}.  The label `$(\backslash1 = \epsilon)$' is an assertion that {\tt \\1} is empty, and `$\backslash1$' represents the whole of {\tt \\1}, if it's non-empty.

Let's match \rx{(a*)b\\1} in {\tt aabaa}.  We start at state~$0$ and go to state~$2$ on {\tt a}, loop on state~$2$ on {\tt a}, and go to state~$3$ on {\tt b}.  At this point, {\tt \\1} represents {\tt aa}.  Since {\tt \\1} is not empty, we may not go from state~$3$ to state~$1$ directly.  We go to state~$4$, since the input matches {\tt aa}, and then to state~$1$.  In fact, the transition from $3$ to $4$ is taken on the first {\tt a} of {\tt aa}, and the automaton is put to sleep for the time the second~{\tt a} is considered.

When a state is put to sleep, it is removed from the state stacks ({\sf mmCurStack} and {\sf mmNextStack}) and is added to {\sf mmSleeping}, a dictionary.  The pertinence of this is that {\sf mmCurStack} and {\sf mmNextStack} are designed to contain at most one occurrence of the same state.  Clearly, sleeping states break this rule.

In the example above, state~$4$ is added to {\sf mmSleeping} at turn~$3$.  Since it has to be woken up at the end of next turn, it is placed under key~$4$.  At the end of turn~$4$, all sleepers are woken up and are integrated into {\sf mmNextState}.

Back-references are not supported in their whole theoretical complexity.  For example, \rx{a*(a*)b\\1b} matches {\tt aabb} in theory, but not in practice.  Why?  Because when {\tt aab} is matched, the automaton has already decided that {\tt \\1} means {\tt aa}, even though it could had been {\tt a}~or~$\epsilon$.  Parentheses are greedy and obey a ``longest-leftmost match'' rule (look for {\sf isBetterCapture} in the source code).

\bye
